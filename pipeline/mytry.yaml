apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: try-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.6, pipelines.kubeflow.org/pipeline_compilation_time: '2021-07-28T20:04:39.870938',
    pipelines.kubeflow.org/pipeline_spec: '{"name": "try"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.6}
spec:
  entrypoint: try
  templates:
  - name: clean
    container:
      args: [--text, /tmp/inputs/text/data, --output-text, /tmp/outputs/output_text/data,
        --o2, /tmp/outputs/o2/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef clean(text_path,output_text_path,o2):\n    import numpy as np\n    import\
        \ pandas as pd\n    import nltk,re\n    #nltk.download('punkt')\n    synopses\
        \ = pd.read_csv(text_path)[:100]\n    synopses = np.array(synopses)\n    synopses\
        \ = synopses.ravel()\n    from nltk.stem.snowball import SnowballStemmer\n\
        \    stemmer = SnowballStemmer(\"english\")\n    def tokenize_and_stem(text):\n\
        \    # first tokenize by sentence, then by word to ensure that punctuation\
        \ is caught as it's own token\n        tokens = [word for sent in nltk.sent_tokenize(text)\
        \ for word in nltk.word_tokenize(sent)]\n        filtered_tokens = []\n  \
        \  # filter out any tokens not containing letters (e.g., numeric tokens, raw\
        \ punctuation)\n        for token in tokens:\n            if re.search('[a-zA-Z]',\
        \ token):\n                filtered_tokens.append(token)\n        stems =\
        \ [stemmer.stem(t) for t in filtered_tokens]\n        return stems\n    def\
        \ tokenize_only(text):\n    # first tokenize by sentence, then by word to\
        \ ensure that punctuation is caught as it's own token\n        tokens = [word.lower()\
        \ for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n\
        \        filtered_tokens = []\n    # filter out any tokens not containing\
        \ letters (e.g., numeric tokens, raw punctuation)\n        for token in tokens:\n\
        \            if re.search('[a-zA-Z]', token):\n                filtered_tokens.append(token)\n\
        \        return filtered_tokens\n    totalvocab_stemmed = []\n    totalvocab_tokenized\
        \ = []\n    for i in synopses:\n        allwords_stemmed = tokenize_and_stem(i)\
        \ #for each item in 'synopses', tokenize/stem\n        totalvocab_stemmed.extend(allwords_stemmed)\
        \ #extend the 'totalvocab_stemmed' list   \n        allwords_tokenized = tokenize_only(i)\n\
        \        totalvocab_tokenized.extend(allwords_tokenized)\n    data={'stemmed':totalvocab_stemmed}\n\
        \    frame=pd.DataFrame(data)\n    frame.to_csv(output_text_path,index=False)\n\
        \    data={'tokenized':totalvocab_tokenized}\n    frame=pd.DataFrame(data)\n\
        \    frame.to_csv(o2,index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Clean',\
        \ description='')\n_parser.add_argument(\"--text\", dest=\"text_path\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-text\"\
        , dest=\"output_text_path\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--o2\", dest=\"o2\",\
        \ type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = clean(**_parsed_args)\n"
      image: star16231108/python:3.7
    inputs:
      artifacts:
      - {name: read-output_text, path: /tmp/inputs/text/data}
    outputs:
      artifacts:
      - {name: clean-o2, path: /tmp/outputs/o2/data}
      - {name: clean-output_text, path: /tmp/outputs/output_text/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.6
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--text", {"inputPath": "text"}, "--output-text", {"outputPath":
          "output_text"}, "--o2", {"outputPath": "o2"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef clean(text_path,output_text_path,o2):\n    import numpy
          as np\n    import pandas as pd\n    import nltk,re\n    #nltk.download(''punkt'')\n    synopses
          = pd.read_csv(text_path)[:100]\n    synopses = np.array(synopses)\n    synopses
          = synopses.ravel()\n    from nltk.stem.snowball import SnowballStemmer\n    stemmer
          = SnowballStemmer(\"english\")\n    def tokenize_and_stem(text):\n    #
          first tokenize by sentence, then by word to ensure that punctuation is caught
          as it''s own token\n        tokens = [word for sent in nltk.sent_tokenize(text)
          for word in nltk.word_tokenize(sent)]\n        filtered_tokens = []\n    #
          filter out any tokens not containing letters (e.g., numeric tokens, raw
          punctuation)\n        for token in tokens:\n            if re.search(''[a-zA-Z]'',
          token):\n                filtered_tokens.append(token)\n        stems =
          [stemmer.stem(t) for t in filtered_tokens]\n        return stems\n    def
          tokenize_only(text):\n    # first tokenize by sentence, then by word to
          ensure that punctuation is caught as it''s own token\n        tokens = [word.lower()
          for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n        filtered_tokens
          = []\n    # filter out any tokens not containing letters (e.g., numeric
          tokens, raw punctuation)\n        for token in tokens:\n            if re.search(''[a-zA-Z]'',
          token):\n                filtered_tokens.append(token)\n        return filtered_tokens\n    totalvocab_stemmed
          = []\n    totalvocab_tokenized = []\n    for i in synopses:\n        allwords_stemmed
          = tokenize_and_stem(i) #for each item in ''synopses'', tokenize/stem\n        totalvocab_stemmed.extend(allwords_stemmed)
          #extend the ''totalvocab_stemmed'' list   \n        allwords_tokenized =
          tokenize_only(i)\n        totalvocab_tokenized.extend(allwords_tokenized)\n    data={''stemmed'':totalvocab_stemmed}\n    frame=pd.DataFrame(data)\n    frame.to_csv(output_text_path,index=False)\n    data={''tokenized'':totalvocab_tokenized}\n    frame=pd.DataFrame(data)\n    frame.to_csv(o2,index=False)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Clean'', description='''')\n_parser.add_argument(\"--text\",
          dest=\"text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-text\",
          dest=\"output_text_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--o2\", dest=\"o2\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = clean(**_parsed_args)\n"], "image":
          "star16231108/python:3.7"}}, "inputs": [{"name": "text"}], "name": "Clean",
          "outputs": [{"name": "output_text"}, {"name": "o2"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "834226dbc47444bbf8e94c5fd2ef33efb4f63ab13b3296ac1a8b115133de00cc", "url":
          "../components/clean.yaml"}'}
  - name: read
    container:
      args: [--output-text, /tmp/outputs/output_text/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def read(output_text_path):
            from sshtunnel import SSHTunnelForwarder
            import pymysql
            import pandas as pd
            import numpy as np
            f=open(output_text_path,'w');
            server=SSHTunnelForwarder(('47.92.240.36', 22),  ssh_username='jt',  ssh_password='vdaubCp7yaSreqlT', remote_bind_address=('192.168.0.84', 3306))
            server.start()
            conn = pymysql.connect(host='127.0.0.1',  port=server.local_bind_port,user='root',  passwd='root' , charset ='utf8',   database = 'dump',)
            sql_get_total = f"select title from ms_paper_ghl_20201026"
            df = pd.read_sql(sql_get_total, con=conn)
            df.to_csv(output_text_path,index=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='Read', description='')
        _parser.add_argument("--output-text", dest="output_text_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = read(**_parsed_args)
      image: star16231108/python:3.7
    outputs:
      artifacts:
      - {name: read-output_text, path: /tmp/outputs/output_text/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.6
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--output-text", {"outputPath": "output_text"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef read(output_text_path):\n    from sshtunnel import SSHTunnelForwarder\n    import
          pymysql\n    import pandas as pd\n    import numpy as np\n    f=open(output_text_path,''w'');\n    server=SSHTunnelForwarder((''47.92.240.36'',
          22),  ssh_username=''jt'',  ssh_password=''vdaubCp7yaSreqlT'', remote_bind_address=(''192.168.0.84'',
          3306))\n    server.start()\n    conn = pymysql.connect(host=''127.0.0.1'',  port=server.local_bind_port,user=''root'',  passwd=''root''
          , charset =''utf8'',   database = ''dump'',)\n    sql_get_total = f\"select
          title from ms_paper_ghl_20201026\"\n    df = pd.read_sql(sql_get_total,
          con=conn)\n    df.to_csv(output_text_path,index=False)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Read'', description='''')\n_parser.add_argument(\"--output-text\",
          dest=\"output_text_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = read(**_parsed_args)\n"], "image": "star16231108/python:3.7"}}, "name":
          "Read", "outputs": [{"name": "output_text"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "1306fc6174aa0e6a9cb6464710dd1e66e09207925559f55d8a82ecbaf5639e70", "url":
          "../components/read.yaml"}'}
  - name: tag
    container:
      args: [--text, /tmp/inputs/text/data, --output-text, /tmp/outputs/output_text/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef tag(text_path,output_text_path):\n    import pandas as pd\n    import\
        \ numpy as np\n    import nltk\n    import re\n    synopses=pd.read_csv(text_path)[:100]\n\
        \    synopses=synopses = np.array(synopses)\n    synopses = synopses.ravel()\n\
        \    def tokenize_and_stem(text):\n    # first tokenize by sentence, then\
        \ by word to ensure that punctuation is caught as it's own token\n       \
        \ tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n\
        \        filtered_tokens = []\n    # filter out any tokens not containing\
        \ letters (e.g., numeric tokens, raw punctuation)\n        for token in tokens:\n\
        \            if re.search('[a-zA-Z]', token):\n                filtered_tokens.append(token)\n\
        \        stems = [stemmer.stem(t) for t in filtered_tokens]\n        return\
        \ stems\n    from sklearn.feature_extraction.text import TfidfVectorizer\n\
        #define vectorizer parameters\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english',\n\
        \                                 use_idf=True, tokenizer=tokenize_and_stem)\n\
        #%time\uFF1ATime execution of a Python statement or expression. https://ipython.readthedocs.io/en/stable/interactive/magics.html\n\
        #%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer\
        \ to synopses\n    tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Tag', description='')\n\
        _parser.add_argument(\"--text\", dest=\"text_path\", type=str, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-text\", dest=\"\
        output_text_path\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = tag(**_parsed_args)\n"
      image: star16231108/python:3.7
    inputs:
      artifacts:
      - {name: read-output_text, path: /tmp/inputs/text/data}
    outputs:
      artifacts:
      - {name: tag-output_text, path: /tmp/outputs/output_text/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.6
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--text", {"inputPath": "text"}, "--output-text", {"outputPath":
          "output_text"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef tag(text_path,output_text_path):\n    import
          pandas as pd\n    import numpy as np\n    import nltk\n    import re\n    synopses=pd.read_csv(text_path)[:100]\n    synopses=synopses
          = np.array(synopses)\n    synopses = synopses.ravel()\n    def tokenize_and_stem(text):\n    #
          first tokenize by sentence, then by word to ensure that punctuation is caught
          as it''s own token\n        tokens = [word for sent in nltk.sent_tokenize(text)
          for word in nltk.word_tokenize(sent)]\n        filtered_tokens = []\n    #
          filter out any tokens not containing letters (e.g., numeric tokens, raw
          punctuation)\n        for token in tokens:\n            if re.search(''[a-zA-Z]'',
          token):\n                filtered_tokens.append(token)\n        stems =
          [stemmer.stem(t) for t in filtered_tokens]\n        return stems\n    from
          sklearn.feature_extraction.text import TfidfVectorizer\n#define vectorizer
          parameters\n    tfidf_vectorizer = TfidfVectorizer(stop_words=''english'',\n                                 use_idf=True,
          tokenizer=tokenize_and_stem)\n#%time\uff1aTime execution of a Python statement
          or expression. https://ipython.readthedocs.io/en/stable/interactive/magics.html\n#%time
          tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer
          to synopses\n    tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Tag'', description='''')\n_parser.add_argument(\"--text\",
          dest=\"text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-text\",
          dest=\"output_text_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = tag(**_parsed_args)\n"], "image": "star16231108/python:3.7"}}, "inputs":
          [{"name": "text"}], "name": "Tag", "outputs": [{"name": "output_text"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "597bb310fc8455262972b01317013e142caba1c3fc5d606afbb5579facb54d3b",
          "url": "../components/tag.yaml"}'}
  - name: try
    dag:
      tasks:
      - name: clean
        template: clean
        dependencies: [read]
        arguments:
          artifacts:
          - {name: read-output_text, from: '{{tasks.read.outputs.artifacts.read-output_text}}'}
      - {name: read, template: read}
      - name: tag
        template: tag
        dependencies: [read]
        arguments:
          artifacts:
          - {name: read-output_text, from: '{{tasks.read.outputs.artifacts.read-output_text}}'}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
